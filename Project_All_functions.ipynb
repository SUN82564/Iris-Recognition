{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris Localization:\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def irislocalization(image):\n",
    "    #image_gray\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    _,image_binary = cv2.threshold(image,60,65,cv2.THRESH_BINARY)\n",
    "    \n",
    "    #find the darket point - pupil\n",
    "    Yp = np.argmin(image_binary.mean(axis = 1))\n",
    "    Xp = np.argmin(image_binary.mean(axis = 0))\n",
    "\n",
    "    #Crop the image to smaller sizes\n",
    "    crop_img = image[max(0,Yp-100):min(Yp+100,320), max(0,Xp-100):min(Xp+100,280)]\n",
    "    \n",
    "    #Pupil Area\n",
    "    area120 = image[max(0, Yp-60):min(Yp+60, 320), max(0, Xp-60):min(Xp+60, 280)]\n",
    "    \n",
    "    mask1 = np.where(image_binary>0,1,0)\n",
    "\n",
    "    miny = np.argmin(mask1.mean(axis = 1))\n",
    "    minx = np.argmin(mask1.mean(axis = 0))\n",
    "\n",
    "    radius1 = np.sum(mask1[miny] == 0) / 2\n",
    "    radius2 = np.sum(mask1 == 0,axis=0)[minx] / 2\n",
    "    radius = np.mean([radius1, radius2])\n",
    "        \n",
    "    crop_img_copy2 = crop_img.copy()\n",
    "    \n",
    "    #Find the inner circles, some times when the radius range is too small it fails to draw the inner circle\n",
    "    for i in range(1,7):\n",
    "        circles_inner = cv2.HoughCircles(area120,cv2.HOUGH_GRADIENT,1,250,\n",
    "                            param1=50,param2=10,minRadius=int(radius)-i,\n",
    "                                 maxRadius=int(radius)+i)\n",
    "        if type(circles_inner) == type(None):\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    circles_inner = np.uint16(np.around(circles_inner))\n",
    "    \n",
    "    \n",
    "    #Find the outer circle\n",
    "    circles_outer = cv2.HoughCircles(crop_img,cv2.HOUGH_GRADIENT,1,250,\n",
    "                            param1=40,param2=10,minRadius=int(radius)+15, maxRadius=118)\n",
    "\n",
    "    circles_outer = np.uint16(np.around(circles_outer))\n",
    "\n",
    "    #Draw Inner\n",
    "    for i in circles_inner[0,:]:\n",
    "        cv2.circle(crop_img_copy2,(int(i[0]+40),int(i[1]+40)),int(i[2]),(0,255,0),2) #plus 40 to map to the crop_img\n",
    "        inner_circle = [int(i[0]+40),int(i[1]+40),int(i[2])]\n",
    "\n",
    "    #Draw Outer\n",
    "    for i in circles_outer[0,:]:\n",
    "        cv2.circle(crop_img_copy2,(i[0],i[1]),i[2],(0,255,0),2)\n",
    "        outer_circle = [i[0],i[1],i[2]]\n",
    "    \n",
    "    return (crop_img, inner_circle, outer_circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris Normalization:\n",
    "def irisnormalization(crop_img, inner_circle, outer_circle):\n",
    "    M = 64\n",
    "    N = 512\n",
    "    \n",
    "    row,column = crop_img.shape\n",
    "    \n",
    "    Image_norm = np.zeros((64,512))\n",
    "\n",
    "    [x1, y1, r_inner] = [inner_circle[0], inner_circle[1], inner_circle[2]]\n",
    "    [x2, y2, r_outer] = [outer_circle[0], outer_circle[1], outer_circle[2]]\n",
    "\n",
    "    #distance between the center of pupil to center of the iris\n",
    "    distance = np.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "\n",
    "    #angle between the center of pupil to center of the iris\n",
    "    theta_diff = np.arctan2(y2-y1, x2-x1)\n",
    "\n",
    "    #using law of cosines to find the distance between the center of pupil to the outer point\n",
    "    diff_dist_square = distance**2 + r_outer**2 - 2*distance*r_outer*np.cos(theta_diff)\n",
    "    diff_dist = diff_dist_square**0.5\n",
    "    \n",
    "    #Using the centroid of pupil as the reference point\n",
    "    for Y in range(0, M):\n",
    "        for X in range(0, N):\n",
    "            theta = 2*np.pi*X/N\n",
    "            x_inner = x1+r_inner*np.cos(theta)\n",
    "            y_inner = y1+r_inner*np.sin(theta)\n",
    "            x_outer = x1 + diff_dist * np.cos(theta)\n",
    "            y_outer = y1 + diff_dist * np.sin(theta)\n",
    "            x = x_inner + (x_outer - x_inner)*Y/M\n",
    "            x = int(x)\n",
    "            x = min(column-1,x) or max(0,x)\n",
    "            y = y_inner + (y_outer - y_inner)*Y/M\n",
    "            y = int(y)\n",
    "            y = min(row-1,y) or max(0,y)\n",
    "            Image_norm[Y, 511-X] = crop_img[y,x]\n",
    "            \n",
    "    return Image_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Enhancement\n",
    "def imgenhancement(Image_norm):\n",
    "    \n",
    "    image2en = np.array(Image_norm,dtype=np.uint8)\n",
    "    img_histEql = cv2.equalizeHist(image2en)\n",
    "\n",
    "    #Detection of reflections and eyelashes, then use as a mask to do logic operation\n",
    "    _,image_noises = cv2.threshold(Image_norm, 178,255, cv2.THRESH_BINARY)\n",
    "    image_noises = np.array(image_noises,dtype=np.uint8)\n",
    "    #plt.imshow(image_reflection, cmap = 'gray')\n",
    "\n",
    "    #masks:\n",
    "    mask1 = cv2.bitwise_not(image_noises)\n",
    "    mask1 = np.array(mask1,dtype=np.uint8)\n",
    "    #image_reflection = np.array(image_reflection,dtype=np.uint8)\n",
    "\n",
    "    noise_removed = cv2.bitwise_and(img_histEql,img_histEql, mask = mask1)\n",
    "    \n",
    "    return noise_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction:\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "# how should we determine the frequency value\n",
    "sigx_1 = 3\n",
    "sigy_1 = 1.5\n",
    "sigx_2 = 4.5\n",
    "sigy_2 = 1.5\n",
    "f = 1/sigy_1   # how should we determine the frequency value\n",
    "\n",
    "kernel_1 = np.zeros((9,9))\n",
    "kernel_2 = np.zeros((9,9))\n",
    "\n",
    "#The modulating function for the defined filter\n",
    "def mod_defined(x,y,f):\n",
    "    return np.cos(2*np.pi*f*np.sqrt(x**2 + y**2))\n",
    "\n",
    "#The gabor filter function\n",
    "def G(x,y,sigx,sigy):\n",
    "    a = 1/(2*np.pi*sigx*sigy)\n",
    "    b = np.exp(-0.5*(x**2/sigx**2 + y**2/sigy**2))*mod_defined(x,y,f)\n",
    "    return a*b\n",
    "\n",
    "#getting the filters for 2 channels\n",
    "for row in range(9):\n",
    "    for col in range(9):\n",
    "        kernel_1[row,col] = G(col,row,sigx_1,sigy_1)\n",
    "        kernel_2[row,col] = G(col,row,sigx_2,sigy_2)\n",
    "        \n",
    "#Getting the feature vectors for the filtered two images\n",
    "def feature_vector(noise_removed):\n",
    "    roi = noise_removed[0:48,:]\n",
    "    vector = []\n",
    "    block_horizontal = int(len(roi[0])/8)\n",
    "    block_vertical = int(len(roi)/8)\n",
    "    for i in range(block_horizontal):\n",
    "        for j in range(block_vertical): #specified the the block\n",
    "            roi_block = roi[j*8:(j+1)*8,i*8:(i+1)*8]\n",
    "            filtered_roi_1 = scipy.signal.convolve2d(roi_block, kernel_1, mode = 'same')\n",
    "            filtered_roi_1 = np.absolute(filtered_roi_1) #as stated in the paper, use absolute values to calculate mean and std\n",
    "            mean = filtered_roi_1.mean()\n",
    "            vector.append(mean)\n",
    "            stdev = filtered_roi_1.std()\n",
    "            vector.append(stdev)\n",
    "            \n",
    "    #for the second channel        \n",
    "    for i in range(block_horizontal):\n",
    "        for j in range(block_vertical): #specified the the block\n",
    "            roi_block = roi[j*8:(j+1)*8,i*8:(i+1)*8]\n",
    "            filtered_roi_2 = scipy.signal.convolve2d(roi_block, kernel_2, mode = 'same')\n",
    "            filtered_roi_2 = np.absolute(filtered_roi_2)\n",
    "            mean = filtered_roi_2.mean()\n",
    "            vector.append(mean)\n",
    "            stdev = filtered_roi_2.std()\n",
    "            vector.append(stdev)\n",
    "            \n",
    "    return vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotations\n",
    "def rotation(image):\n",
    "    templates = []\n",
    "    #degrees = [-9,-6,-3,0,3,6,9]  #of initial image\n",
    "    degrees = [-3,-2,-1,0,1,2,3]\n",
    "    for i in degrees:\n",
    "        loc_norm = int(512*i/360)\n",
    "        if i > 0:\n",
    "            rotated = np.hstack([image[:,loc_norm:],image[:,:loc_norm]])\n",
    "        else:  #case when loc norm < 0\n",
    "            rotated = np.hstack([image[:,(512 + int(loc_norm)):],image[:,:(512 + int(loc_norm))]] )\n",
    "        \n",
    "        templates.append(rotated)\n",
    "    \n",
    "    return templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning_data has rotations\n",
    "\n",
    "def trainning():\n",
    "    vec_4_all = []\n",
    "    vec_4_set = []\n",
    "    vec_4_pic = []\n",
    "    \n",
    "    file_main_Path = '/Users/sunflower/Desktop/GR5293_Image_Analysis/iris/'#001/1/\n",
    "    sets = []\n",
    "    #try to get the image sets and find the right path\n",
    "    for i in range(1,109):\n",
    "        data = '00' + str(i)\n",
    "        if len(data) == 4:\n",
    "            data = data[1:]\n",
    "        if len(data) == 5:\n",
    "            data = data[2:]\n",
    "        else:\n",
    "            data\n",
    "        sets.append(data) \n",
    "    sets = sets\n",
    "    \n",
    "    for img_set in sets:\n",
    "        #print(img_set)\n",
    "        filePath_set = file_main_Path + img_set\n",
    "        \n",
    "        for image_num in range(1,4):\n",
    "            num = str(image_num)\n",
    "            img_name = img_set + '_' + '1' + '_' + num +'.bmp'\n",
    "            folder_image = '/1/'+ img_name\n",
    "            filePath = filePath_set + '/1/'+ img_name\n",
    "            #print(filePath)\n",
    "            img = cv2.imread(filePath)\n",
    "            folder_image =''\n",
    "            \n",
    "            crop_img, inner_circle, outer_circle = irislocalization(img)\n",
    "            image_norm = irisnormalization(crop_img, inner_circle, outer_circle)\n",
    "            templates = rotation(image_norm)\n",
    "            \n",
    "            for each_tem in templates:\n",
    "                noise_removed = imgenhancement(each_tem)\n",
    "                enhanced_img = imgenhancement(noise_removed)\n",
    "                feature = feature_vector(noise_removed)\n",
    "                vec_4_pic.append(feature)\n",
    "            \n",
    "            #vec_4_set.append(vec_4_pic)\n",
    "            \n",
    "       # vec_4_all.append(vec_4_set)\n",
    "        \n",
    "    return vec_4_pic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    vec_4_all = []\n",
    "    vec_4_set = []\n",
    "    \n",
    "    file_main_Path = '/Users/sunflower/Desktop/GR5293_Image_Analysis/iris/'#001/1/\n",
    "    sets = []\n",
    "    #try to get the image sets and find the right path\n",
    "    for i in range(1,109):\n",
    "        data = '00' + str(i)\n",
    "        if len(data) == 4:\n",
    "            data = data[1:]\n",
    "        if len(data) == 5:\n",
    "            data = data[2:]\n",
    "        else:\n",
    "            data\n",
    "        sets.append(data) \n",
    "    sets = sets\n",
    "    \n",
    "    for img_set in sets:\n",
    "        #print(img_set)\n",
    "        filePath_set = file_main_Path + img_set\n",
    "        \n",
    "        for image_num in range(1,5):\n",
    "            num = str(image_num)\n",
    "            img_name = img_set + '_' + '2' + '_' + num +'.bmp'\n",
    "            folder_image = '/2/'+ img_name\n",
    "            filePath = filePath_set + '/2/'+ img_name\n",
    "            #print(filePath)\n",
    "            img = cv2.imread(filePath)\n",
    "            \n",
    "            crop_img, inner_circle, outer_circle = irislocalization(img)\n",
    "            image_norm = irisnormalization(crop_img, inner_circle, outer_circle)\n",
    "            noise_removed = imgenhancement(image_norm)\n",
    "            enhanced_img = imgenhancement(noise_removed)\n",
    "            feature = feature_vector(noise_removed)\n",
    "            \n",
    "            folder_image =''\n",
    "            vec_4_set.append(feature)\n",
    "            \n",
    "        #vec_4_all.append(vec_4_set)\n",
    "        \n",
    "    return vec_4_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = trainning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris matching:\n",
    "#return accuracy rate, L1,L2,Cosine distance for table\n",
    "from scipy.spatial import distance\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "#distance.euclidean : L1\n",
    "#distance.sqeuclidean : L2\n",
    "#distance.cosine : Cosine\n",
    "def irismatching(trainingdata, testingdata, dimension):\n",
    "\n",
    "    trainingdata = np.array(trainingdata)\n",
    "    trainingdata = np.where(np.isnan(trainingdata), 0, trainingdata)\n",
    "\n",
    "    testingdata = np.array(testingdata)\n",
    "    testingdata = np.where(np.isnan(testingdata), 0, testingdata)\n",
    "\n",
    "    sets = np.arange(1,109)\n",
    "\n",
    "    temp_train_match_to = np.repeat(sets, 21)\n",
    "\n",
    "    train_match_to = np.repeat(sets,3) #subject to change\n",
    "\n",
    "    testing_matching_to = np.repeat(sets, 4) #subject to change\n",
    "\n",
    "#model\n",
    "    clf = LDA(n_components = dimension)\n",
    "    clf.fit(trainingdata, temp_train_match_to)  #only takes <=2 dimension data\n",
    "\n",
    "#LDA\n",
    "    training_reduced = clf.transform(trainingdata)\n",
    "    testing_reduced = clf.transform(testingdata)\n",
    "\n",
    "    predict_result = []                    #predict picture beglong to which set\n",
    "    L1_predict_result = []\n",
    "    L2_predict_result = []\n",
    "    \n",
    "    for st in range(np.shape(testingdata)[0]):  #for each testing picture\n",
    "        shortest_distances = []                 #to find the each training picture that has the shortest distance with it\n",
    "        L1_shortest_distances =[]\n",
    "        L2_shortest_distances = []\n",
    "        \n",
    "        test_lda = testing_reduced[st]          #corresponding LDA of the vector\n",
    "\n",
    "        for pic in range(int(np.shape(trainingdata)[0]/7)):    #each picture in the training set\n",
    "            cos_distances = []                          #cos distances of each templates for the picture\n",
    "            L1_distances = []\n",
    "            L2_distances = []\n",
    "            \n",
    "            for tem in range(7):           #calculated the distiance between feature vector of picture set and a picture template\n",
    "                L1 = distance.euclidean(test_lda, training_reduced[pic*7+tem])\n",
    "                L2 = distance.sqeuclidean(test_lda,training_reduced[pic*7+tem])\n",
    "                cos = distance.cosine(test_lda, training_reduced[pic*7+tem])\n",
    "\n",
    "                cos_distances.append(cos)   #distances from each templates\n",
    "                L1_distances.append(L1)\n",
    "                L2_distances.append(L2)\n",
    "                \n",
    "            shortest_distances.append(min(cos_distances))  #get the shortest distances among templates; use as picture distance\n",
    "            L1_shortest_distances.append(min(L1_distances))\n",
    "            L2_shortest_distances.append(min(L2_distances))\n",
    "            \n",
    "        shortest_dist_loc = np.argmin(shortest_distances)  #most similar pictures\n",
    "        L1_shortest_dist_loc = np.argmin(L1_shortest_distances)\n",
    "        L2_shortest_dist_loc = np.argmin(L2_shortest_distances)\n",
    "        \n",
    "        predict_result.append(train_match_to[shortest_dist_loc])\n",
    "        L1_predict_result.append(train_match_to[L1_shortest_dist_loc])\n",
    "        L2_predict_result.append(train_match_to[L2_shortest_dist_loc])\n",
    "        \n",
    "    predict_result = np.array(predict_result, dtype = np.int)\n",
    "    L1_predict_result = np.array(L1_predict_result, dtype = np.int)\n",
    "    L2_predict_result = np.array(L2_predict_result, dtype = np.int)\n",
    "    \n",
    "    cos_accuracy = 1-sum(predict_result != testing_matching_to)/len(testing_matching_to)\n",
    "    L1_accuracy = 1-sum(L1_predict_result != testing_matching_to)/len(testing_matching_to)\n",
    "    L2_accuracy = 1-sum(L2_predict_result != testing_matching_to)/len(testing_matching_to)\n",
    "    \n",
    "    return cos_accuracy, L1_accuracy, L2_accuracy, Predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance Evaluation:\n",
    "import pandas\n",
    "\n",
    "def CRR_Curve(training, testing):\n",
    "    \n",
    "    dimension = [50, 60, 70, 80, 90, 100, 107]\n",
    "    rate = np.zeros(7)\n",
    "    rate_L1 = np.zeros(7)\n",
    "    rate_L2 = np.zeros(7)\n",
    "    for i in range(7):\n",
    "        cos_accuracy, L1_accuracy, L2_accuracy, _, = irismatching(training, testing, dimension[i])\n",
    "        rate[i] = cos_accuracy\n",
    "        rate_L1[i] = L1_accuracy\n",
    "        rate_L2[i] = L2_accuracy\n",
    "        \n",
    "    plt.subplot(1,3,1)   \n",
    "    plt.plot(dimension, rate)\n",
    "    plt.title('CRR of cosine distance measure')\n",
    "    plt.xlabel('Dimensionality of the feature vector')\n",
    "    plt.ylabel('correct recognition rate')\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(dimension, rate_L1)\n",
    "    plt.title('CRR of L1 distance measure')\n",
    "    plt.xlabel('Dimensionality of the feature vector')\n",
    "    plt.ylabel('correct recognition rate')\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(dimension, rate_L2)\n",
    "    plt.title('CRR of L2 distance measure')\n",
    "    plt.xlabel('Dimensionality of the feature vector')\n",
    "    plt.ylabel('correct recognition rate')\n",
    "    \n",
    "    plt.show\n",
    "\n",
    "def similarity_measure_table(training, testing):\n",
    "    \n",
    "    cos_1, L1_1, L2_1, _, = irismatching(training, testing, 107)\n",
    "    \n",
    "    cos, L1, L2, _, = irimatching(training, testing, 100)\n",
    "    \n",
    "    table_data = [[L1_1,L1],[L2_1,L2],[cos_1,cos]]\n",
    "    \n",
    "    table_data = pandas.DataFrame(table_data)\n",
    "    \n",
    "    table_data.index = ['L1 Distance Measure', 'L2 Distance Measure', 'Cosine Distance Measure']\n",
    "    table_data.columns = ['Original Feature Set', 'Reduced Feature Set']\n",
    "    \n",
    "    print(table_data)\n",
    "    \n",
    "#def falsematch(training, testing):\n",
    "\n",
    "#def falsenonmatch(training, testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris matching for roc:\n",
    "#return accuracy rate, L1,L2,Cosine distance for table\n",
    "from scipy.spatial import distance\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "#distance.euclidean : L1\n",
    "#distance.sqeuclidean : L2\n",
    "#distance.cosine : Cosine\n",
    "\n",
    "def matching_roc(trainingdata, testingdata, dimension):\n",
    "    trainingdata = np.array(trainingdata)\n",
    "    trainingdata = np.where(np.isnan(trainingdata), 0, trainingdata)\n",
    "\n",
    "    testingdata = np.array(testingdata)\n",
    "    testingdata = np.where(np.isnan(testingdata), 0, testingdata)\n",
    "\n",
    "    sets = np.arange(1,109)\n",
    "\n",
    "    temp_train_match_to = np.repeat(sets, 21)\n",
    "\n",
    "    train_match_to = np.repeat(sets,3) #subject to change\n",
    "\n",
    "    testing_matching_to = np.repeat(sets, 4) #subject to change\n",
    "\n",
    "    #model\n",
    "    dimension = 107\n",
    "    clf = LDA(n_components = dimension)\n",
    "    clf.fit(trainingdata, temp_train_match_to)  #only takes <=2 dimension data\n",
    "    #clf.score(testingdata, testing_matching_to)\n",
    "    prediction = clf.predict(testingdata)\n",
    "    y_scores = clf.predict_proba(testingdata)[:,1]\n",
    "    return testing_matching_to, prediction, y_scores\n",
    "\n",
    "def new_prediction(actual, prediction, y_scores, threshold):\n",
    "   \n",
    "    FalseP = 0\n",
    "    FalseN = 0\n",
    "    \n",
    "    new = []\n",
    "    for i in range(len(y_scores)):\n",
    "        if y_scores[i] < threshold:\n",
    "            new.append(0)    #non-match\n",
    "        else:\n",
    "            new.append(1)    #accepted\n",
    "    \n",
    "    for i in range(len(y_scores)):\n",
    "        if actual[i] != prediction[i] and new[i] == 1: #actual != prediction but accepted\n",
    "            FalseP +=1\n",
    "        if actual[i] == prediction[i] and new[i] == 0: #actual == prediction but threshollded to be non-matching\n",
    "            FalseN +=1\n",
    "    \n",
    "    TotalP = sum(new)\n",
    "    TotalN = len(new) - sum(new)\n",
    "    \n",
    "    FMR = FalseP/TotalP\n",
    "    FRR = FalseN/TotalN\n",
    "    return FMR, FRR\n",
    "\n",
    "\n",
    "def get_roc(trainging, testing):\n",
    "    dimension = 107\n",
    "    testing_matching_to, prediction, y_scores = matching_roc(trainingdata, testingdata, dimension)\n",
    "    \n",
    "    threshold = [0.446, 0.472, 0.502]\n",
    "    \n",
    "    table  = []\n",
    "    for i in threshold:\n",
    "        FMR, FRR = new_prediction(testing_matching_to, prediction, y_scores, i)\n",
    "        table.append([i,FMR,FRR])\n",
    "        \n",
    "    table = pandas.DataFrame(table)\n",
    "    table.columns = ['Threshold', 'False Match Rate', 'False Non Match Rate']\n",
    "    \n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Threshold  False Match Rate  False Non Match Rate\n",
      "0      0.446               0.0              0.759907\n",
      "1      0.472               0.0              0.759907\n",
      "2      0.502               0.0              0.759907\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "get_roc(trainingdata, testingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Original Feature Set  Reduced Feature Set\n",
      "L1 Distance Measure                  0.768519             0.761574\n",
      "L2 Distance Measure                  0.768519             0.761574\n",
      "Cosine Distance Measure              0.803241             0.789352\n"
     ]
    }
   ],
   "source": [
    "similarity_measure_table(trainingdata, testingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_falses(dist_mtx, threshold):  # dist_mtx each row is the dist of 1 test img to all train imgs\n",
    "    gt_acceptance = np.zeros_like(dist_mtx)\n",
    "    for r in range(len(gt_acceptance)):\n",
    "        idx = r // 4\n",
    "        gt_acceptance[r][idx * 3: (idx + 1) * 3] = 1\n",
    "    accept = dist_mtx <= threshold\n",
    "    false_accept = (1 - gt_acceptance) * accept\n",
    "    false_reject = gt_acceptance * (1 - accept)\n",
    "    FAE = false_accept.sum() / (dist_mtx.shape[0] * dist_mtx.shape[1])\n",
    "    FRE = false_reject.sum() / (dist_mtx.shape[0] * dist_mtx.shape[1])\n",
    "    return FAE, FRE\n",
    "\n",
    "\n",
    "def plot_roc(dist_mtx):\n",
    "    thresholds = np.linspace(dist_mtx.min(), dist_mtx.max(), 50)\n",
    "\n",
    "    FAEs = []\n",
    "    FREs = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        a, b = compute_falses(dist_mtx, t)\n",
    "        FAEs.append(a)\n",
    "        FREs.append(b)\n",
    "\n",
    "    roc_table = pd.DataFrame({\"Threshold\": thresholds,\n",
    "                              \"FAE\": FAEs,\n",
    "                              \"FRE\": FREs})\n",
    "    roc_table.to_csv(\"ROCTable.csv\")\n",
    "\n",
    "    plt.plot(np.array(FAEs), np.array(FREs), color='navy', linestyle='-')\n",
    "    plt.xlabel('False Match Rate')\n",
    "    plt.ylabel('False Non_match Rate')\n",
    "    plt.title('FMR ROC Curve')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
